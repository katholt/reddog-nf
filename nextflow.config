manifest {
  description = 'RedDog mapping pipeline, nextflow implementation'
  author = 'Nextflow implementation: Stephen Watts; Original pipeline: David Edwards, Bernie Pope, Kat Holt'
  homePage = 'https://github.com/scwatts/reddog-nf'
  nextflowVersion = '>=20.01.0'
}

params {
  reads = 'data/multiple_replicons/reads/*_{1,2}.fastq.gz'
  reference = 'data/multiple_replicons/NCTC13753.gbk'
  output_dir = 'output'
  run_info_dir = "${output_dir}/run_info/"
  help = false
}

profiles {
  standard {
    process {
      executor = 'local'
      cpus = 4
    }
  }

  massive {
    process {
      executor = 'slurm'
      maxRetries = 3
      cpus = 1
      memory = { 2.GB * task.attempt }
      time = { 30.minutes * task.attempt }
      clusterOptions = {
        account = 'js66'
        qos = task.time <= 30.minutes ? 'shortq' : 'normal'
        partition = task.time <= 30.minutes ? 'short,comp' : 'comp'
        return "--account=${account} --qos=${qos} --partition=${partition}"
      }

      // NOTE: we must use the `val` attribute on the DataflowVariable object
      // (isolate_count) here as the call is blocking and causes issues if in
      // the main pipeline script. Calling within closures is fine as the result
      // at this point has been evaluated.

      withName: prepare_reference {
        executor = 'local'
      }

      withName: create_mpileups {
        time = { 10.minutes * task.attempt }
      }

      withName: gene_coverage_depth {
        time = { 10.minutes + (isolate_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_count.val * 20).MB }
      }

      withName: call_snps {
        time = { 10.minutes * task.attempt }
      }

      withName: calculate_replicon_statistics {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_replicon_statistics {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_snp_sites {
        time = { 10.minutes + (isolate_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_count.val * 10).MB }
      }

      withName: create_allele_matrix {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_allele_matrices {
        time = { 10.minutes + (isolate_passing_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 10).MB }
      }

      withName: create_snp_alignment {
        time = { 10.minutes + (isolate_passing_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 20).MB }
      }

      withName: determine_coding_consequences {
        time = { 10.minutes + (isolate_passing_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 20).MB }
      }

      withName: infer_phylogeny {
        time = { 10.minutes + (isolate_passing_count.val * 2).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 20).MB }
      }
    }
  }
}

dag {
  enabled = true
  file = "${params.run_info_dir}/dag.svg"
}

report {
  enabled = true
  file = "${params.run_info_dir}/report.html"
}

timeline {
  enabled = true
  file = "${params.run_info_dir}/timeline.html"
}

trace {
  enabled = true
  file = "${params.run_info_dir}/trace.txt"
}

// For whatever reason I can't access task.processor.name here so instead we do
// a string sub to achieve desired result
executor.$slurm.jobName = { "rr-${task.name}".replace(' ', '_') }
