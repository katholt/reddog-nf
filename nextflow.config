manifest {
  description = 'RedDog mapping pipeline, nextflow implementation'
  author = 'Nextflow implementation: Stephen Watts; Original pipeline: David Edwards, Bernie Pope, Kat Holt'
  homePage = 'https://github.com/scwatts/reddog-nf'
  nextflowVersion = '>=20.01.0'
}

params {
  // Input and output
  reads = 'tests/3_simulated_reads/reads/*{1,2}.fastq'
  reference = 'tests/3_simulated_reads/data/reference.gbk'
  output_dir = 'output'
  run_info_dir = "${output_dir}/run_info/"

  // Merge run settings
  merge_run = false
  previous_run_dir = ''

  // Optional stages
  subsample_reads = false
  subsample_read_count = 100000
  quality_assessment = true
  force_tree = false

  // Executor options
  // Maximum jobs to submit to the SLURM queue at once
  queue_size = 100
  // Number of processors to use for local execution
  processors = 4

  // Misc
  force = false
  help = false
}

profiles {
  standard {
    process {
      executor = 'local'
    }
  }

  massive {
    process {
      executor = 'slurm'

      // Retry jobs - typically with more resources
      errorStrategy = 'retry'
      maxRetries = 3

      // Required for consistent resumes on massives NFS
      cache = 'lenient'

      // Default job resources
      cpus = 1
      memory = { 2.GB * task.attempt }
      time = { 30.minutes * task.attempt }

      // Set options absent from nf slurm api
      clusterOptions = {
        account = 'js66'
        qos = task.time <= 30.minutes ? 'shortq' : 'normal'
        partition = task.time <= 30.minutes ? 'short,comp' : 'comp'
        return "--account=${account} --qos=${qos} --partition=${partition}"
      }

      // NOTE: we must use the `val` attribute on the DataflowVariable object
      // (isolate_count) here as the call is blocking and causes issues if in
      // the main pipeline script. Calling within closures is fine as the result
      // at this point has been evaluated.

      withName: prepare_reference {
        executor = 'local'
      }

      withName: create_mpileups {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_read_quality_reports {
        time = { 10.minutes + (isolate_count.val * 0.5).minutes }
      }

      withName: gene_coverage_depth {
        time = { 10.minutes + (isolate_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_count.val * 20).MB }
      }

      withName: call_snps {
        time = { 10.minutes * task.attempt }
      }

      withName: calculate_mapping_statistics {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_mapping_statistics {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_snp_sites {
        time = { 10.minutes + (isolate_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_count.val * 10).MB }
      }

      withName: create_allele_matrix {
        time = { 10.minutes * task.attempt }
      }

      withName: aggregate_allele_matrices {
        time = { 10.minutes + (isolate_passing_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 10).MB }
      }

      withName: create_snp_alignment {
        time = { 10.minutes + (isolate_passing_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 20).MB }
      }

      withName: determine_coding_consequences {
        time = { 10.minutes + (isolate_passing_count.val * 0.5).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 20).MB }
      }

      withName: infer_phylogeny {
        time = { 10.minutes + (isolate_passing_count.val * 2).minutes }
        memory = { 1024.MB + (isolate_passing_count.val * 20).MB }
      }
    }
  }
}

dag {
  enabled = true
  file = "${params.run_info_dir}/dag.svg"
}

report {
  enabled = true
  file = "${params.run_info_dir}/report.html"
}

timeline {
  enabled = true
  file = "${params.run_info_dir}/timeline.html"
}

trace {
  enabled = true
  file = "${params.run_info_dir}/trace.txt"
}

// SLURM options
// Replace 'nf' in SLURM job name with 'rr'
// For whatever reason I can't access task.processor.name here so instead we do
// a string sub to achieve desired result
executor.$slurm.jobName = { "rr-${task.name}".replace(' ', '_') }
// Limit queue size (100 is default)
executor.$slurm.queueSize = params.queue_size

// Local executor options
executor.$local.queueSize = params.processors
